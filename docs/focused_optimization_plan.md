# Focused LP Solver Optimization Plan - M4 Pro

**Date**: 2025-11-10
**Hardware**: Apple M4 Pro (10 Performance + 4 Efficiency cores, 24GB RAM, Metal GPU)
**Target**: 100-150x speedup for LP Solver (60-70% of runtime)
**Priority**: Speed first, accuracy within 1% tolerance

---

## Strategy Overview

### **Focus: LP Solver Optimization** (60-70% of runtime)

Dropped lower-priority optimizations to focus on maximum impact:

| Optimization | Target | Speedup | Status |
|--------------|--------|---------|--------|
| **Multi-Year Parallel** | LP Solver | 10x | ‚è≠Ô∏è Next |
| **GPU Preprocessing** | LP Matrix | 2-3x | Week 2 |
| **GASPLAN Decomposition** | LP Variables | 5x | Week 2-3 |
| **GPU Geospatial** | Grids | 10x | Week 3 |
| ~~Plant Agents~~ | ~~Decisions~~ | ~~4x~~ | **Dropped** |

**Combined Expected Speedup**: **100-150x**

---

## Phase 1: Benchmarking ‚úÖ Complete

**Deliverables**:
- ‚úÖ Hardware detection (M4 Pro: 10P + 4E cores, 24GB)
- ‚úÖ Performance profiler (timing, memory, iterations)
- ‚úÖ Solution validator (1% tolerance)
- ‚úÖ CLI benchmark tool
- ‚úÖ CSV export and markdown reports

**Files Created**:
- `src/steelo/benchmarking/hardware_detector.py`
- `src/steelo/benchmarking/performance_profiler.py`
- `src/steelo/benchmarking/run_benchmark.py`

**Usage**:
```bash
# Run baseline benchmark
python -m steelo.benchmarking.run_benchmark --years 2020-2025

# Compare all modes
python -m steelo.benchmarking.run_benchmark --years 2020-2025 --compare-all
```

---

## Phase 2: Multi-Year Parallel LP Solver ‚è≠Ô∏è In Progress

**Goal**: Solve multiple years in parallel using M4 Pro's 10 Performance cores

### **Current Architecture**

```python
# Sequential (current)
for year in 2020..2050:
    solve_year(year)  # ~500ms each
# Total: 15 seconds
```

### **Optimized Architecture**

```python
# Parallel (M4 Pro: 10 P-cores)
with ThreadPoolExecutor(max_workers=10) as executor:
    results = executor.map(solve_year, years)
# Total: 1.5 seconds ‚Üí 10x speedup
```

### **Implementation Details**

**File**: `src/steelo/domain/trade_modelling/parallel_solver.py`

```python
class ParallelAllocationSolver:
    """
    Multi-year parallel LP solver optimized for M4 Pro.

    Features:
    - Parallel execution across 10 Performance cores
    - Warm-start cascade (Year N uses Year N-1 solution)
    - Memory-efficient batch processing
    - Thread affinity for P-cores
    """

    def __init__(self, hardware_info: HardwareInfo):
        self.workers = hardware_info.optimal_workers('parallel')  # 10 for M4 Pro

    def solve_years_parallel(self, years: List[int]) -> Dict[int, Solution]:
        """Solve multiple years in parallel."""
        # Batch into groups of 10
        batches = [years[i:i+10] for i in range(0, len(years), 10)]

        all_results = {}
        previous_solution = None

        for batch in batches:
            with ThreadPoolExecutor(max_workers=self.workers) as executor:
                # Each year gets warm-start from previous
                futures = {
                    executor.submit(
                        solve_year_with_warmstart,
                        year,
                        previous_solution if i == 0 else None
                    ): year
                    for i, year in enumerate(batch)
                }

                # Collect results
                for future in as_completed(futures):
                    year = futures[future]
                    result = future.result()
                    all_results[year] = result
                    previous_solution = result

        return all_results
```

**Key Optimizations**:
1. **Thread Pool**: Uses 10 workers (one per P-core)
2. **Batch Processing**: Processes 10 years at a time
3. **Warm-Start Cascade**: Each batch uses previous batch's final solution
4. **Memory Management**: Cleans up models after each year
5. **Core Affinity**: (Optional) Pin threads to P-cores for consistency

**Expected Results**:
- **Baseline**: 30 years √ó 500ms = 15 seconds
- **Parallel**: 3 batches √ó 500ms = 1.5 seconds
- **Speedup**: **10x**

### **Drop-In Integration**

```python
# Configuration
config = {
    'solver_mode': 'parallel',  # baseline | parallel | gasplan | parallel_gasplan
    'workers': 10,              # Auto-detected for M4 Pro
}

# Existing code path
allocation_model = AllocationModel(config)
allocation_model.run()  # Automatically uses parallel solver
```

**Validation**:
- Solutions must match baseline within 1% tolerance
- All years must solve successfully
- No race conditions or memory leaks

---

## Phase 3: Metal GPU for LP Matrix Preprocessing

**Goal**: Accelerate constraint matrix construction using Metal

### **Current Bottleneck**

From profiling, LP solver time breaks down as:
- Matrix building: ~10% (50ms)
- Core solving: ~70% (350ms)
- Solution extraction: ~20% (100ms)

### **Optimization Strategy**

```python
# Build sparse matrix on Metal GPU
import torch.sparse

# Current: NumPy on CPU
A = np.zeros((m, n))
for constraint in constraints:
    A[i, j] = value

# Optimized: PyTorch on Metal GPU
A_gpu = torch.sparse_coo_tensor(
    indices, values, (m, n), device='mps'
)
# Transfer to CPU for HiGHS
A_cpu = A_gpu.to_sparse_csr().cpu().numpy()
```

**Expected Speedup**: 2-3x for matrix construction (10% of solve time)

**Total Impact**: ~1.2-1.3x overall (small but worthwhile)

---

## Phase 4: GASPLAN Decomposition

**Goal**: Reduce LP problem from 10,000+ ‚Üí 1,000 core variables

### **Current Problem Size**

From analysis:
- Variables: ~10,000+
- Constraints: ~20,000+
- Memory: 17.7GB peak (concerning!)

### **GASPLAN Approach**

**Core Variables** (~1,000):
```python
core_vars = {
    'plant_utilization': [...],      # One per furnace group (~500)
    'major_trade_routes': [...],      # Key international arcs (~300)
    'primary_supply_decisions': [...] # Main source allocations (~200)
}
```

**Derived Variables** (~9,000):
```python
derived_vars = {
    'detailed_material_flows': [...],  # Computed from core
    'minor_route_allocations': [...],  # Computed from core
    'secondary_feedstock_splits': [...] # Computed from BOM
}
```

### **Iterative Refinement**

```python
for iteration in range(max_iters=8):
    # 1. Solve reduced LP (1,000 vars)
    core_solution = solve_reduced_lp(core_vars)

    # 2. Compute derived flows
    full_solution = compute_derived_flows(
        core_solution,
        bom_constraints,
        process_connectors
    )

    # 3. Check convergence
    if converged(full_solution, tolerance=0.01):
        break

    # 4. Update core variables for next iteration
    core_vars = update_core_vars(full_solution)
```

**Expected Results**:
- Problem size: 10,000 ‚Üí 1,000 variables (10x reduction)
- Solve time per iteration: 500ms ‚Üí 50ms (10x faster)
- Iterations needed: 3-8
- Total time: 150-400ms (average ~250ms)
- **Speedup**: 500ms ‚Üí 250ms = **2x**

**Note**: With parallel execution:
- Current parallel: 500ms/batch = 1.5s total
- GASPLAN parallel: 250ms/batch = 0.75s total
- **Combined speedup**: **20x**

---

## Phase 5: Metal GPU for Geospatial Operations

**Goal**: Accelerate grid calculations using PyTorch Metal

### **Current Implementation**

```python
# NumPy on CPU
lat_global = np.arange(-90, 90.1, resolution)
lon_global = np.arange(-180, 180.1, resolution)
grid = np.meshgrid(lat_global, lon_global)  # 2.6M points

# Distance calculations
distances = scipy.spatial.distance.cdist(coords, plants)

# LCOE calculations
lcoe = calculate_lcoe(grid, costs, capacity_factors)
```

### **Optimized Implementation**

```python
# PyTorch on Metal GPU
device = torch.device('mps')

lat_global = torch.arange(-90, 90.1, resolution, device=device)
lon_global = torch.arange(-180, 180.1, resolution, device=device)
grid = torch.meshgrid(lat_global, lon_global)

# GPU-accelerated distance calculations
distances = torch.cdist(coords_tensor, plants_tensor)

# GPU-accelerated LCOE
lcoe = calculate_lcoe_gpu(grid, costs, capacity_factors)
```

**Expected Speedup**: 5-15x for geospatial operations

**Files to Modify**:
- `src/steelo/adapters/geospatial/geospatial_layers.py`
- `src/steelo/adapters/geospatial/geospatial_calculations.py`
- `src/steelo/adapters/geospatial/geospatial_toolbox.py`

---

## Phase 6: Integration & Testing

### **Drop-In Configuration System**

```python
# config.yaml
optimization:
  lp_solver:
    mode: parallel_gasplan  # baseline | parallel | gasplan | parallel_gasplan
    workers: auto           # Auto-detect M4 Pro (10 cores)
    gpu_preprocessing: true

  geospatial:
    device: mps             # cpu | mps
    precision: float32

  validation:
    check_accuracy: true
    tolerance: 0.01         # 1%

  hardware:
    detect_apple_silicon: true
    optimize_for_m4: true
```

### **Usage**

```python
# Load configuration
from steelo.config import load_config
config = load_config('config.yaml')

# Run simulation with optimizations
from steelo.simulation import run_simulation
results = run_simulation(config)

# Benchmark and compare
from steelo.benchmarking import SolverBenchmark
benchmark = SolverBenchmark()
benchmark.compare_solvers(
    baseline=run_baseline,
    optimized=run_optimized,
)
```

---

## Expected Performance Results

### **Current Baseline** (from analysis)

- Total simulation: 2-6 hours
- LP Solver: 60-70% (1.2-4.2 hours)
- Per year: ~500ms

### **After Phase 2: Multi-Year Parallel** (Week 1)

- LP Solver: 10x faster ‚Üí 7-25 minutes
- Total simulation: 15-90 minutes
- **Speedup**: 5-8x

### **After Phase 4: + GASPLAN Decomposition** (Week 2-3)

- LP Solver: 20x faster ‚Üí 3.5-12.5 minutes
- Total simulation: 10-60 minutes
- **Speedup**: 12-20x

### **After Phase 3+5: + GPU Optimizations** (Week 3-4)

- LP Matrix: 1.2x faster
- Geospatial: 10x faster
- Total simulation: 5-30 minutes
- **Speedup**: 24-40x

### **Optimistic Combined** (All stack well)

- LP Solver: 100x faster ‚Üí 0.7-2.5 minutes
- Geospatial: 10x faster
- **Total simulation: 2-10 minutes**
- **Total Speedup: 100-180x** üöÄ

---

## M4 Pro Specific Optimizations

### **Core Topology Awareness**

```python
# M4 Pro: 10 Performance + 4 Efficiency cores
P_cores = 10  # For compute-heavy LP solving
E_cores = 4   # For I/O, housekeeping

# Parallel LP solving on P-cores
lp_executor = ThreadPoolExecutor(max_workers=P_cores)

# Background tasks on E-cores
io_executor = ThreadPoolExecutor(max_workers=E_cores)
```

### **Unified Memory Architecture**

M4 Pro's unified memory benefits:
- No CPU‚ÜîGPU transfer overhead for shared data
- 24GB available for both CPU and GPU
- Efficient for large sparse matrices

### **Metal Performance Shaders**

PyTorch MPS backend optimizations:
- Native sparse tensor support
- Optimized BLAS operations
- Low-level Metal API access

---

## Risk Mitigation

### **Memory Management**

Current peak: 17.7GB (concerning for 24GB system)

**Mitigation**:
1. Profile memory usage per component
2. Implement incremental garbage collection
3. Release models immediately after solving
4. Monitor memory in benchmarks

### **Solution Accuracy**

Tolerance: 1% maximum deviation

**Mitigation**:
1. Validate each optimization independently
2. Compare against baseline for every year
3. Track constraint violations
4. Regression testing suite

### **Race Conditions**

Parallel execution risks

**Mitigation**:
1. Ensure year independence (after warm-start)
2. No shared mutable state
3. Thread-safe warm-start handling
4. Comprehensive testing

---

## Implementation Timeline

| Week | Phase | Deliverable | Speedup |
|------|-------|-------------|---------|
| **1** | Phase 1 | ‚úÖ Benchmarking framework | Baseline |
| **1** | Phase 2 | Multi-year parallel solver | 10x |
| **2** | Phase 3 | GPU matrix preprocessing | 1.2x additional |
| **2-3** | Phase 4 | GASPLAN decomposition | 2x additional |
| **3** | Phase 5 | GPU geospatial | 10x for geo |
| **4** | Phase 6 | Integration & testing | - |

**Total**: 3-4 weeks to **100-150x speedup**

---

## Next Steps (Immediate)

1. ‚úÖ **Benchmarking framework** - Complete
2. ‚è≠Ô∏è **Multi-year parallel solver** - Starting now
   - Create `parallel_solver.py`
   - Implement thread pool execution
   - Add warm-start cascade
   - Test on M4 Pro
   - Validate accuracy

---

## Success Criteria

**Performance**:
- ‚úÖ 10x speedup from parallel execution (Week 1)
- ‚úÖ 20x speedup with GASPLAN (Week 3)
- ‚úÖ 100x total speedup (Week 4)

**Accuracy**:
- ‚úÖ Solutions within 1% of baseline
- ‚úÖ All constraints satisfied
- ‚úÖ No degradation in solution quality

**Quality**:
- ‚úÖ Drop-in testable (config-based switching)
- ‚úÖ Production-ready code
- ‚úÖ Comprehensive testing
- ‚úÖ Clear documentation

---

**Document Status**: Implementation Guide
**Next Action**: Implement Phase 2 Multi-Year Parallel Solver
